import os
import torch
import torch.nn as nn
import torch.multiprocessing as mp
import torch.distributed as dist
from torch._subclasses.fake_tensor import FakeTensor, FakeTensorMode
import torch_dist_ext


envs = {  
    "HIP_VISIBLE_DEVICES": "0,1,2,3,4,5,6,7",
}
for k,v in envs.items():
    os.environ[k] = v


class RMSNorm(nn.Module):
    def __init__(self, dim, norm_eps=1e-6, dtype=torch.float):
        super().__init__()
        self.eps = norm_eps
        self.weight = nn.Parameter(torch.randn(dim, dtype=dtype), requires_grad=False)

    def forward(self, x):
        input_dtype = x.dtype
        variance = x.float().pow(2).mean(-1, keepdim=True)
        x = x * torch.rsqrt(variance + self.eps)
        x = x.to(input_dtype)
        return self.weight * x


def setup(rank, world_size):
    torch.cuda.set_device(rank)
    dist.init_process_group(
        backend='nccl',
        init_method='tcp://127.0.0.1:23459',
        rank=rank,
        world_size=world_size)


def worker(rank, world_size, allreduce_in, residual_in, rms, ref_residual_out, ref_norm_out, eps, use_fused=True):
    setup(rank, world_size)
    torch_dist_ext.setup_env(rank, world_size)
    num_tokens, hidden_dim = residual_in.shape
    local_allreduce_in = allreduce_in[rank].cuda(rank)
    local_residual_in = residual_in.cuda(rank)
    local_rms = rms.cuda(rank)
    torch.cuda.synchronize()
    dist.barrier()
    prof = torch.profiler.profile(
            activities=[
                torch.profiler.ProfilerActivity.CPU,
                torch.profiler.ProfilerActivity.CUDA,
            ])
    with prof:
        if not use_fused:
            dist.all_reduce(local_allreduce_in)    
            local_norm_out = local_rms(local_allreduce_in + local_residual_in)
        else:
            local_norm_out, local_residual_out = torch_dist_ext.allreduce_rms(rank, world_size, local_allreduce_in, local_residual_in, 
                local_rms.weight.data, eps, torch_dist_ext.get_workspace(local_allreduce_in))
    maxdiff = (local_norm_out.cpu() - ref_norm_out).abs().max()
    print(f"rank:{rank}, maxdiff:{maxdiff}")
    # assert torch.allclose(local_norm_out.cpu(), ref_norm_out)
    if rank == 0:
        print(prof.key_averages().table(sort_by='cuda_time_total', row_limit=10000))
    dist.destroy_process_group()


def main():
    def testcase(world_size=4, num_tokens=128, hidden_dim=1024, eps=1e-6, dtype=torch.float):
        allreduce_in = torch.randn(world_size, num_tokens, hidden_dim, dtype=dtype)
        residual_in = torch.randn(num_tokens, hidden_dim, dtype=dtype)
        rms = RMSNorm(hidden_dim, dtype=dtype)
        ref_residual_out = allreduce_in.sum(dim=0) + residual_in
        ref_norm_out = rms(ref_residual_out)
        mp.spawn(worker, args=(world_size, allreduce_in, residual_in, rms, ref_residual_out, ref_norm_out, eps), nprocs=world_size, join=True)
    testcase(dtype=torch.float)
    testcase(dtype=torch.bfloat16)
    testcase(dtype=torch.half)


if __name__ == '__main__':
    main()
